# ğŸ”‘ Liveness, Readiness, Resource Limits, Env. Variables

## Liveness Probes

Bazen containerâ€™larÄ±n iÃ§erisinde Ã§alÄ±ÅŸan uygulamalar, tam anlamÄ±yla doÄŸru Ã§alÄ±ÅŸmayabilir. Ã‡alÄ±ÅŸan uygulama Ã§Ã¶kmemiÅŸ, kapanmamÄ±ÅŸ ama aynÄ± zamanda tam iÅŸlevini yerine getirmiyorsa kubelet bunu tespit edemiyor.

Liveness, sayesinde containerâ€™a **bir request gÃ¶ndererek, TCP connection aÃ§arak veya container iÃ§erisinde bir komut Ã§alÄ±ÅŸtÄ±rarak** doÄŸru Ã§alÄ±ÅŸÄ±p Ã§alÄ±ÅŸmadÄ±ÄŸÄ±nÄ± anlayabiliriz.

_AÃ§Ä±klama kod iÃ§erisinde_ :arrow\_down:

```shell
# http get request gÃ¶nderelim.
# eÄŸer 200 ve Ã¼zeri cevap dÃ¶nerse baÅŸarÄ±lÄ±!
# dÃ¶nmezse kubelet container'Ä± yeniden baÅŸlatacak.
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-http
spec:
  containers:
  - name: liveness
    image: k8s.gcr.io/liveness
    args:
    - /server
    livenessProbe:
      httpGet:	# get request'i gÃ¶nderiyoruz.
        path: /healthz # path tanÄ±mÄ±
        port: 8080 # port tanÄ±mÄ±
        httpHeaders: # get request'imize header eklemek istersek
        - name: Custom-Header
          value: Awesome
      initialDelaySeconds: 3 # uygulama hemen ayaÄŸa kalkmayabilir,
      											 # Ã§alÄ±ÅŸtÄ±ktan x sn sonra isteÄŸi gÃ¶nder.
      periodSeconds: 3 # kaÃ§ sn'de bir bu istek gÃ¶nderilecek. 
      								 # (healthcheck test sÃ¼rekli yapÄ±lÄ±r.)
---
# uygulama iÃ§erisinde komut Ã§alÄ±ÅŸtÄ±ralÄ±m.
# eÄŸer exit -1 sonucu alÄ±nÄ±rsa container baÅŸtan baÅŸlatÄ±lÄ±r.
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-exec
spec:
  containers:
  - name: liveness
    image: k8s.gcr.io/busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
    livenessProbe:
      exec:  			# komut Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r.
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5
---
# tcp connection yaratalÄ±m. EÄŸer baÅŸarÄ±lÄ±ysa devam eder, yoksa 
# container baÅŸtan baÅŸlatÄ±lÄ±r.
apiVersion: v1
kind: Pod
metadata:
  name: goproxy
  labels:
    app: goproxy
spec:
  containers:
  - name: goproxy
    image: k8s.gcr.io/goproxy:0.1
    ports:
    - containerPort: 8080
    livenessProbe:	# tcp connection yaratÄ±lÄ±r.
      tcpSocket:
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 20
```

## Readiness Probes

![](<.gitbook/assets/image (23).png>)

#### **Ã–rnek Senaryo**

3 podumuz ve 1 LoadBalancer serviceâ€™imiz var. Bir gÃ¼ncelleme yaptÄ±k; yeni bir image oluÅŸturduk. Eski podlar devreden Ã§Ä±ktÄ±, yenileri alÄ±ndÄ±. Yenileri alÄ±ndÄ±ÄŸÄ±ndan itibaren LoadBalancer gelen trafiÄŸi yÃ¶nlendirmeye baÅŸlayacaktÄ±r. Peki, benim uygulamalarÄ±m ilk aÃ§Ä±ldÄ±ÄŸÄ±nda bir yere baÄŸlanÄ±p bir data Ã§ekip, bunu iÅŸliyor ve sonra Ã§alÄ±ÅŸmaya baÅŸlÄ±yorsa? Bu sÃ¼re zarfÄ±nda gelen requestler doÄŸru cevaplanamayacaktÄ±r. KÄ±sacasÄ±, uygulamamÄ±z Ã§alÄ±ÅŸÄ±yor ama hizmet sunmaya hazÄ±r deÄŸil.

â€“> **Kubelet,** bir containerÄ±n ne zaman trafiÄŸi kabul etmeye (Initial status) hazÄ±r olduÄŸunu bilmek iÃ§in **Readiness Probes** kullanÄ±r. Bir Poddaki tÃ¼m containerâ€™lar Readiness Probes kontrolÃ¼nden onay alÄ±rsa **Service Podâ€™un arkasÄ±na eklenir.**

YukarÄ±daki Ã¶rnekte, yeni imageâ€™lar oluÅŸturulurken eski Podâ€™lar hemen **terminate** edilmez. Ã‡Ã¼nkÃ¼, iÃ§erisinde daha Ã¶nceden alÄ±nmÄ±ÅŸ istekler ve bu istekleri iÅŸlemek iÃ§in yÃ¼rÃ¼tÃ¼len iÅŸlemler olabilir. Bu sebeple, k8s Ã¶nce bu Podâ€™un service ile iliÅŸkisini keser ve yeni istekler almasÄ±nÄ± engeller. Ä°Ã§erideki mevcut isteklerinde sonlanmasÄ±nÄ± bekler.

`terminationGracePeriodSconds: 30` â€“> Mevcut iÅŸlemler biter, 30 sn bekler ve kapanÄ±r. (_30sn default ayardÄ±r, gayet yeterlidir._)

**â€“> Readiness ile Liveness arasÄ±ndaki fark, Readiness ilk Ã§alÄ±ÅŸma anÄ±nÄ± baz alÄ±rken, Liveness sÃ¼rekli Ã§alÄ±ÅŸÄ±p Ã§alÄ±ÅŸmadÄ±ÄŸÄ±nÄ± kontrol eder.**

> Ã–rneÄŸin; Backendâ€™in ilk aÃ§Ä±lÄ±ÅŸta MongoDBâ€™ye baÄŸlanmasÄ± iÃ§in geÃ§en bir sÃ¼re vardÄ±r. MongoDB baÄŸlantÄ±sÄ± saÄŸlandÄ±ktan sonraPodâ€™un arkasÄ±na Service eklenmesi mantÄ±klÄ±dÄ±r. **Bu sebeple, burada readinessâ€™i kullanabiliriz.**

AynÄ± Livenessâ€™ta olduÄŸu gibi 3 farklÄ± yÃ¶ntem vardÄ±r:

* **http/get**, **tcp connection** ve **command Ã§alÄ±ÅŸtÄ±rma**.

```shell
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  labels:
    team: development
spec:
  replicas: 3
  selector:
    matchLabels:
      app: frontend
  template:
    metadata:
      labels:
        app: frontend
    spec:
      containers:
      - name: frontend
        image: ozgurozturknet/k8s:blue
        ports:
        - containerPort: 80
        livenessProbe:
          httpGet:
            path: /healthcheck
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
        readinessProbe:
          httpGet:
            path: /ready	# Bu endpoint'e istek atÄ±lÄ±r, OK dÃ¶nerse uygulama Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r.
            port: 80
          initialDelaySeconds: 20 # BaÅŸlangÄ±Ã§tan 20 sn gecikmeden sonra ilk kontrol yapÄ±lÄ±r.
          periodSeconds: 3 # 3sn'de bir denemeye devam eder.
          terminationGracePeriodSconds: 50 # YukarÄ±da yazÄ±ldÄ± aÃ§Ä±klamasÄ±.
---
apiVersion: v1
kind: Service
metadata:
  name: frontend
spec:
  selector:
    app: frontend
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
```

## Resource Limits

\-> Podâ€™larÄ±n CPU ve Memory kÄ±sÄ±tlamalarÄ±nÄ± yÃ¶netmemizi saÄŸlar. Aksini belirtmediÄŸimiz sÃ¼rece K8s Ã¼zerinde Ã§alÄ±ÅŸtÄ±ÄŸÄ± makinenin CPU ve Memoryâ€™sini %100 kullanabilir. Bu durum bir sorun oluÅŸturur. Bu sebeple Podâ€™larÄ±n ne kadar CPU ve Memory kullanacaÄŸÄ±nÄ± belirtebiliriz.

### CPU TanÄ±mÄ±

![](<.gitbook/assets/image (25).png>)

### Memory TanÄ±mÄ±

![](.gitbook/assets/image-20211230020644173.png)

### YAML DosyasÄ±nda TanÄ±m

```shell
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: requestlimit
  name: requestlimit
spec:
  containers:
  - name: requestlimit
    image: ozgurozturknet/stress
    resources:
      requests: # Podun Ã§alÄ±ÅŸmasÄ± iÃ§in en az gereken gereksinim
        memory: "64M"	# Bu podu en az 64M 250m (yani Ã§eyrek core)
        cpu: "250m" # = Ã‡eyrek CPU core = "0.25"
      limits: # Podun Ã§alÄ±ÅŸmasÄ± iÃ§in en fazla gereken limit
        memory: "256M"
        cpu: "0.5" # = "YarÄ±m CPU Core" = "500m"
```

â€“> EÄŸer gereksinimler saÄŸlanamazsa **container oluÅŸturulamaz.**

â€“> Memory, CPUâ€™ya gÃ¶re farklÄ± Ã§alÄ±ÅŸÄ±yor. K8s iÃ§erisinde memoryâ€™nin limitlerden fazla deÄŸer istediÄŸinde engellemesi gibi bir durum yok. EÄŸer memory, limitlerden fazlasÄ±na ihtiyaÃ§ duyarsa â€œOOMKilledâ€ durumuna geÃ§erek pod restart edilir.

> **AraÅŸtÄ±rma konusu:** Bir podâ€™un limitlerini ve min. gereksinimlerini neye gÃ¶re belirlemeliyiz?

## Environment Variables

Ã–rneÄŸin, bir node.js sunucusu oluÅŸturduÄŸumuzu ve veritabanÄ± bilgilerini sunucu dosyalarÄ± iÃ§erisinde sakladÄ±ÄŸÄ±mÄ±zÄ± dÃ¼ÅŸÃ¼nelim. EÄŸer, sunucu dosyalarÄ±ndan oluÅŸturduÄŸumuz container imageâ€™Ä± baÅŸka birisinin eline geÃ§erse bÃ¼yÃ¼k bir gÃ¼venlik aÃ§Ä±ÄŸÄ± meydana gelir. Bu sebeple **Environment Variables** kullanmamÄ±z gerekir.

### YAML TanÄ±mlamasÄ±

```shell
apiVersion: v1
kind: Pod
metadata:
  name: envpod
  labels:
    app: frontend
spec:
  containers:
  - name: envpod
    image: ozgurozturknet/env:latest
    ports:
    - containerPort: 80
    env:
      - name: USER   # Ã¶nce name'ini giriyoruz.
        value: "Ozgur"  # sonra value'sunu giriyoruz.
      - name: database
        value: "testdb.example.com"
```

### Pod iÃ§inde tanÄ±mlanmÄ±ÅŸ Env. Var.â€™larÄ± GÃ¶rmek

```shell
kubectl exec <podName> -- printenv
```

## Port-Forward (Local -> Pod)

â€“> Kendi local sunucularÄ±mÄ±zdan istediÄŸimiz k8s clusterâ€™Ä± iÃ§erisindeki objectâ€™lere direkt ulaÅŸabilmek iÃ§in **port-forward** aÃ§abiliriz. Bu objectâ€™i test etmek iÃ§in en iyi yÃ¶ntemlerden biridir.

```shell
kubectl port-forward <objectType>/<podName> <localMachinePort>:<podPort>

kubectl port-forward pod/envpod 8080:80
# Benim cihazÄ±mdaki 8080 portuna gelen tÃ¼m istekleri,bu podun 80 portuna gÃ¶nder.

curl 127.0.0.1:8080
# Test iÃ§in yazabilirsin.
```

_CMD + C yapÄ±ldÄ±ÄŸÄ±nda port-forwarding sona erer._
